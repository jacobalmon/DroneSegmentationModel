{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SementicSegmentationDrone(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, tile_size=512):\n",
    "        self.images = sorted(glob.glob(os.path.join(image_dir, \"*.jpg\")))\n",
    "        self.masks = sorted(glob.glob(os.path.join(mask_dir, \"*.png\")))\n",
    "        self.transform = transform\n",
    "        self.tile_size = tile_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx])\n",
    "        mask = Image.open(self.masks[idx])\n",
    "\n",
    "        image = image.resize((self.tile_size, self.tile_size))\n",
    "        mask = mask.resize((self.tile_size, self.tile_size), Image.NEAREST)\n",
    "\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        if image.size[0] > self.tile_size or image.size[1] > self.tile_size:\n",
    "            image_tiles, mask_tiles = self.split_into_tiles(image, mask)\n",
    "            idx_tile = random.randint(0, len(image_tiles) - 1)\n",
    "            image, mask = image_tiles[idx_tile], mask_tiles[idx_tile]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = torch.tensor(mask, dtype=torch.long)\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "    def split_into_tiles(self, image, mask):\n",
    "        image_width, image_height = image.size\n",
    "        image_tiles = []\n",
    "        mask_tiles = []\n",
    "\n",
    "        for i in range(0, image_width, self.tile_size):\n",
    "            for j in range(0, image_height, self.tile_size):\n",
    "                image_tile = image.crop((i, j, min(i+self.tile_size, image_width), min(j+self.tile_size, image_height)))\n",
    "                mask_pil = Image.fromarray(mask) \n",
    "                mask_tile = mask_pil.crop((i, j, min(i+self.tile_size, image_width), min(j+self.tile_size, image_height)))\n",
    "                image_tiles.append(image_tile)\n",
    "                mask_tiles.append(np.array(mask_tile))\n",
    "\n",
    "        return image_tiles, mask_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = {\n",
    "    \"train_images\": \"advanced_data/x_train\",\n",
    "    \"train_masks\": \"advanced_data/y_train\",\n",
    "    \"val_images\": \"advanced_data/x_valid\",\n",
    "    \"val_masks\": \"advanced_data/y_valid\",\n",
    "    \"test_images\": \"advanced_data/x_test\",\n",
    "    \"test_masks\": \"advanced_data/y_test\"\n",
    "}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = SementicSegmentationDrone(directory[\"train_images\"], directory[\"train_masks\"], transform=transform, tile_size=512)\n",
    "valid_data = SementicSegmentationDrone(directory[\"val_images\"], directory[\"val_masks\"], transform=transform, tile_size=512)\n",
    "test_data = SementicSegmentationDrone(directory[\"test_images\"], directory[\"test_masks\"], transform=transform, tile_size=512)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=4, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "base_model = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "\n",
    "num_classes = 24\n",
    "base_model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model = base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(base_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    base_model.train()\n",
    "    run_loss = 0.0\n",
    "\n",
    "    for images, masks in train_dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = base_model(images)['out']\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        run_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {run_loss/len(train_dataloader)}')\n",
    "\n",
    "    base_model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in valid_dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = base_model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    print(f'Validation Loss after Epoch {epoch+1}: {valid_loss/len(valid_dataloader)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
