{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SementicSegmentationDrone(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, tile_size=512):\n",
    "        self.images = sorted(glob.glob(os.path.join(image_dir, \"*.jpg\")))\n",
    "        self.masks = sorted(glob.glob(os.path.join(mask_dir, \"*.png\")))\n",
    "        self.transform = transform\n",
    "        self.tile_size = tile_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx])\n",
    "        mask = Image.open(self.masks[idx])\n",
    "\n",
    "        image = image.resize((self.tile_size, self.tile_size))\n",
    "        mask = mask.resize((self.tile_size, self.tile_size), Image.NEAREST)\n",
    "\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        if image.size[0] > self.tile_size or image.size[1] > self.tile_size:\n",
    "            image_tiles, mask_tiles = self.split_into_tiles(image, mask)\n",
    "            idx_tile = random.randint(0, len(image_tiles) - 1)\n",
    "            image, mask = image_tiles[idx_tile], mask_tiles[idx_tile]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = torch.tensor(mask, dtype=torch.long)\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "    def split_into_tiles(self, image, mask):\n",
    "        image_width, image_height = image.size\n",
    "        image_tiles = []\n",
    "        mask_tiles = []\n",
    "\n",
    "        for i in range(0, image_width, self.tile_size):\n",
    "            for j in range(0, image_height, self.tile_size):\n",
    "                image_tile = image.crop((i, j, min(i+self.tile_size, image_width), min(j+self.tile_size, image_height)))\n",
    "                mask_pil = Image.fromarray(mask) \n",
    "                mask_tile = mask_pil.crop((i, j, min(i+self.tile_size, image_width), min(j+self.tile_size, image_height)))\n",
    "                image_tiles.append(image_tile)\n",
    "                mask_tiles.append(np.array(mask_tile))\n",
    "\n",
    "        return image_tiles, mask_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = {\n",
    "    \"train_images\": \"advanced_data/x_train\",\n",
    "    \"train_masks\": \"advanced_data/y_train\",\n",
    "    \"val_images\": \"advanced_data/x_valid\",\n",
    "    \"val_masks\": \"advanced_data/y_valid\",\n",
    "    \"test_images\": \"advanced_data/x_test\",\n",
    "    \"test_masks\": \"advanced_data/y_test\"\n",
    "}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = SementicSegmentationDrone(directory[\"train_images\"], directory[\"train_masks\"], transform=transform, tile_size=512)\n",
    "valid_data = SementicSegmentationDrone(directory[\"val_images\"], directory[\"val_masks\"], transform=transform, tile_size=512)\n",
    "test_data = SementicSegmentationDrone(directory[\"test_images\"], directory[\"test_masks\"], transform=transform, tile_size=512)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=4, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/almon004/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/almon004/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DeepLabV3:\n\tUnexpected key(s) in state_dict: \"aux_classifier.0.weight\", \"aux_classifier.1.weight\", \"aux_classifier.1.bias\", \"aux_classifier.1.running_mean\", \"aux_classifier.1.running_var\", \"aux_classifier.1.num_batches_tracked\", \"aux_classifier.4.weight\", \"aux_classifier.4.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load pre-trained DeepLabV3 model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/almon004/DroneSegmentationModel/deeplabv3_model/deeplabv3_resnet101.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m\n\u001b[1;32m     11\u001b[0m base_model\u001b[38;5;241m.\u001b[39mclassifier[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m256\u001b[39m, num_classes, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeepLabV3:\n\tUnexpected key(s) in state_dict: \"aux_classifier.0.weight\", \"aux_classifier.1.weight\", \"aux_classifier.1.bias\", \"aux_classifier.1.running_mean\", \"aux_classifier.1.running_var\", \"aux_classifier.1.num_batches_tracked\", \"aux_classifier.4.weight\", \"aux_classifier.4.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.segmentation as models\n",
    "\n",
    "# Load the DeepLabV3 model with no pre-trained weights\n",
    "base_model = models.deeplabv3_resnet101(pretrained=False, weights_backbone=None)\n",
    "\n",
    "# Load your custom pre-trained model weights\n",
    "state_dict = torch.load('/home/almon004/DroneSegmentationModel/deeplabv3_model/deeplabv3_resnet101.pth')\n",
    "\n",
    "# Filter out the auxiliary classifier keys\n",
    "state_dict = {k: v for k, v in state_dict.items() if 'aux_classifier' not in k}\n",
    "\n",
    "# Load the filtered state dict into the model\n",
    "base_model.load_state_dict(state_dict)\n",
    "\n",
    "# Modify the classifier layer to match your number of classes\n",
    "num_classes = 24\n",
    "base_model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "base_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(base_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    base_model.train()\n",
    "    run_loss = 0.0\n",
    "\n",
    "    for images, masks in train_dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = base_model(images)['out']\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        run_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {run_loss/len(train_dataloader)}')\n",
    "\n",
    "    base_model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in valid_dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = base_model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    print(f'Validation Loss after Epoch {epoch+1}: {valid_loss/len(valid_dataloader)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
